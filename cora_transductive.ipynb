{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/cora_transductive\"\n",
    "max_groupsize = 169\n",
    "min_groupsize = 2\n",
    "num_categories = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(datadir+\"/cora.cites\", nodetype=str) # must be in edgelist (node1 node2\\n) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "classmap = {\"Neural_Networks\":\"1\",\"Case_Based\":\"2\",\"Reinforcement_Learning\":\"3\", \"Probabilistic_Methods\":\"4\", \"Genetic_Algorithms\":\"5\", \"Rule_Learning\":\"6\", \"Theory\": \"7\"}\n",
    "vertex_features = {}\n",
    "vertex_labels = {}\n",
    "with open(datadir+\"/cora.content\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "for line in content:\n",
    "    keyval = line.split(\"\\t\")\n",
    "    feature = []\n",
    "    for val in keyval[1:1434]:\n",
    "        if val==\"0\":\n",
    "            feature.append(0)\n",
    "        if val==\"1\":\n",
    "            feature.append(1)\n",
    "    classes[str(keyval[0])] = classmap[str(keyval[1434])[:-1]]\n",
    "    \n",
    "    label = np.zeros((num_categories,))\n",
    "    label[int(classmap[str(keyval[1434])[:-1]])-1] = 1\n",
    "    vertex_labels[str(keyval[0])] = np.asarray(label)\n",
    "    vertex_features[str(keyval[0])] = np.asarray(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feature_array = []\n",
    "\n",
    "for feature_index in vertex_features:\n",
    "    feature = vertex_features[feature_index]\n",
    "    feature_array.append(feature)\n",
    "pca_pre = np.asarray(feature_array)\n",
    "pca = PCA(n_components = 64)\n",
    "pca.fit(pca_pre)\n",
    "features_new = pca.transform(pca_pre)\n",
    "i = 0\n",
    "for feature_index in vertex_features:\n",
    "    vertex_features[feature_index] = features_new[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dimension = 64\n",
    "\n",
    "def getFeaturesTrainingData():\n",
    "    \n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    for vertex in G.nodes:\n",
    "        vertex_embedding_list = []\n",
    "        lists.append({\"f\":vertex_features[vertex].tolist()})\n",
    "        labels.append(vertex_labels[vertex])\n",
    "    X_unshuffled = []\n",
    "    for hlist in lists:\n",
    "        x = np.zeros((feature_dimension,))\n",
    "        x[:feature_dimension] = hlist[\"f\"]\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X_Features = np.asarray(X_arr)\n",
    "    Y_Features = np.asarray(Y_arr)\n",
    "    return X_Features, Y_Features\n",
    "\n",
    "X_Features, Y_Features = getFeaturesTrainingData()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Features_train, X_Features_test, Y_Features_train, Y_Features_test = train_test_split(X_Features, Y_Features, train_size=0.05, test_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from Models import FeaturesMLP\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "dataset_name = \"cora_transductive\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "FeaturesMLP_model = FeaturesMLP(input_dimension=feature_dimension,\n",
    "                 num_outputs=num_categories,\n",
    "                 dataset_name=dataset_name)\n",
    "plot_model(FeaturesMLP_model, to_file='images/'+dataset_name+'_FeaturesMLP_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/FeaturesMLP_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/FeaturesMLP_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = FeaturesMLP_model.fit(X_Features_train, Y_Features_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "Y_pred_Features = FeaturesMLP_model.predict(X_Features_test, batch_size=16, verbose=1)\n",
    "finals_pred_Features = []\n",
    "finals_test_Features = []\n",
    "for p in Y_pred_Features:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_Features.append(final)\n",
    "\n",
    "for i in Y_Features_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_Features.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_Features, finals_pred_Features, target_names=target_names))\n",
    "print(accuracy_score(finals_test_Features, finals_pred_Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges = {}\n",
    "nodes = G.nodes\n",
    "max_size = 0\n",
    "min_size = 100000\n",
    "for node in nodes:\n",
    "    neighborhood = list(G.neighbors(node))\n",
    "    if len(neighborhood)+1 <= max_groupsize and len(neighborhood)+1 >= min_groupsize:\n",
    "        hyperedges[node] = {}\n",
    "        hyperedges[node][\"members\"] = []\n",
    "        hyperedges[node][\"members\"].append(node)\n",
    "        hyperedges[node][\"category\"] = classes[node]\n",
    "        for neighbor in neighborhood:\n",
    "            hyperedges[node][\"members\"].append(neighbor)\n",
    "    if len(neighborhood)+1 < min_size:\n",
    "        min_size = len(neighborhood)+1\n",
    "    if len(neighborhood) > max_size:\n",
    "        max_size = len(neighborhood)+1\n",
    "print(min_size, max_size)\n",
    "print(len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_amounts = {}\n",
    "cat_names = {\"1\":\"Neural Networks\",\"2\":\"Case Based\",\"3\":\"Reinforcement Learning\", \"4\":\"Probabilistic Methods\", \"5\":\"Genetic Algorithms\", \"6\":\"Rule Learning\", \"7\": \"Theory\"}\n",
    "\n",
    "for h in hyperedges:\n",
    "    if cat_names[str(hyperedges[h][\"category\"])] not in cat_amounts:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]=1\n",
    "    else:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexMemberships = {}\n",
    "for h_index in hyperedges:\n",
    "    hyperedge = hyperedges[h_index]\n",
    "    authors = hyperedge[\"members\"]\n",
    "    for author in authors:\n",
    "        if author in vertexMemberships:\n",
    "            vertexMemberships[author].append(h_index)\n",
    "        else:\n",
    "            authorMembershipList = []\n",
    "            authorMembershipList.append(h_index)\n",
    "            vertexMemberships[author] = authorMembershipList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import SubsampleAndTraverse\n",
    "\n",
    "walksSaT = SubsampleAndTraverse(length=25, \n",
    "                                num_walks=50, \n",
    "                                hyperedges=hyperedges, \n",
    "                                vertexMemberships=vertexMemberships,\n",
    "                                alpha=1., \n",
    "                                beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import TraverseAndSelect\n",
    "\n",
    "walksTaS = TraverseAndSelect(length=25, \n",
    "                             num_walks=50, \n",
    "                             hyperedges=hyperedges, \n",
    "                             vertexMemberships=vertexMemberships,\n",
    "                             alpha=1.,\n",
    "                             beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import EmbedWord2Vec\n",
    "\n",
    "vertex_embedding_dimension = 16\n",
    "hyperedge_embedding_dimension = 128\n",
    "\n",
    "vertex_ids, vertex_embeddings = EmbedWord2Vec(walks=walksSaT,dimension=vertex_embedding_dimension)\n",
    "print(\"Vertex embeddings finished.\")\n",
    "hyperedge_ids, hyperedge_embeddings = EmbedWord2Vec(walks=walksTaS,dimension=hyperedge_embedding_dimension)\n",
    "print(\"Hyperedge embeddings finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list,\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist(),\"f\":vertex_features[h].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((hyperedge_embedding_dimension + vertex_embedding_dimension*max_groupsize + feature_dimension,))\n",
    "        i = 0\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        x[hyperedge_embedding_dimension + vertex_embedding_dimension*max_groupsize:] = hlist[\"f\"]\n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[hyperedge_embedding_dimension + i*embedding.shape[0]:hyperedge_embedding_dimension + (i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = getTrainingData()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# np.save(datadir+'/X_hyperedge_'+str(max_groupsize)+'.npy', X)\n",
    "# np.save(datadir+'/Y_hyperedge_'+str(max_groupsize)+'.npy', Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.1, test_size=0.9)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Models import DeepHyperedgesTransductive\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"cora_transductive\"\n",
    "batch_size = 16\n",
    "num_epochs = 75\n",
    "\n",
    "deephyperedges_transductive_model = DeepHyperedgesTransductive(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                    hyperedge_embedding_dimension=hyperedge_embedding_dimension, \n",
    "                                    feature_dimension=feature_dimension,\n",
    "                                    max_hyperedge_size=max_groupsize,\n",
    "                                    num_outputs=num_categories,\n",
    "                                    dataset_name=dataset_name)\n",
    "plot_model(deephyperedges_transductive_model, to_file='images/'+dataset_name+'_deephyperedges_transductive_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/deephyperedges_transductive_weights.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/deephyperedges_transductive_logs', \n",
    "                         histogram_freq=0, batch_size=batch_size, write_graph=True, \n",
    "                         write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = deephyperedges_transductive_model.fit(X_train, Y_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "\n",
    "y_pred = deephyperedges_transductive_model.predict(X_test, batch_size=16, verbose=1)\n",
    "finals_pred = []\n",
    "finals_test = []\n",
    "for p in y_pred:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred.append(final)\n",
    "\n",
    "for i in Y_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test, finals_pred, target_names=target_names,digits=4))\n",
    "print(accuracy_score(finals_test, finals_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLPTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        lists.append({\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist(), \"f\": vertex_features[h].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    for hlist in lists:\n",
    "        x = np.zeros((hyperedge_embedding_dimension + feature_dimension,))\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        x[hyperedge_embedding_dimension:] = hlist[\"f\"]\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X_MLP = np.asarray(X_arr)\n",
    "    Y_MLP = np.asarray(Y_arr)\n",
    "    return X_MLP, Y_MLP\n",
    "\n",
    "X_MLP, Y_MLP = getMLPTrainingData()\n",
    "# np.save(datadir+'/X_MLP_'+str(max_groupsize)+'.npy', X_MLP)\n",
    "# np.save(datadir+'/Y_MLP_'+str(max_groupsize)+'.npy', Y_MLP)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_MLP_transductive_train, X_MLP_transductive_test, Y_MLP_transductive_train, Y_MLP_transductive_test = train_test_split(X_MLP, Y_MLP, train_size=0.1, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from Models import MLPTransductive\n",
    "dataset_name = \"cora_transductive\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "MLP_transductive_model = MLPTransductive(hyperedge_embedding_dimension=hyperedge_embedding_dimension,\n",
    "                feature_dimension=feature_dimension,\n",
    "                num_outputs=num_categories,\n",
    "                dataset_name=dataset_name)\n",
    "plot_model(MLP_transductive_model, to_file='images/'+dataset_name+'_MLP_transductive_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/MLP_transductive_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/MLP_transductive_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = MLP_transductive_model.fit(X_MLP_transductive_train, Y_MLP_transductive_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "Y_pred_MLP = MLP_transductive_model.predict(X_MLP_transductive_test, batch_size=16, verbose=1)\n",
    "finals_pred_MLP = []\n",
    "finals_test_MLP = []\n",
    "for p in Y_pred_MLP:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_MLP.append(final)\n",
    "\n",
    "for i in Y_MLP_transductive_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_MLP.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_MLP, finals_pred_MLP, target_names=target_names,digits=4))\n",
    "print(accuracy_score(finals_test_MLP, finals_pred_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDSTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list,\"f\": vertex_features[h].tolist()})\n",
    "        lists.append\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((vertex_embedding_dimension*max_groupsize + feature_dimension,))\n",
    "        x[vertex_embedding_dimension*max_groupsize:] = hlist[\"f\"]\n",
    "        i = 0        \n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[i*embedding.shape[0]:(i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X_deepset, Y_deepset = getDSTrainingData()\n",
    "\n",
    "print(X_deepset.shape)\n",
    "print(Y_deepset.shape)\n",
    "\n",
    "# np.save(datadir+'/X_deepset_'+str(max_groupsize)+'.npy', X_deepset)\n",
    "# np.save(datadir+'/Y_deepset_'+str(max_groupsize)+'.npy', Y_deepset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_deepset_transductive_train, X_deepset_transductive_test, Y_deepset_transductive_train, Y_deepset_transductive_test = train_test_split(X_deepset, Y_deepset, train_size=0.2365, test_size=0.7635)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from Models import DeepSetsTransductive\n",
    "dataset_name = \"cora_transductive\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "Deepsets_transductive_model = DeepSetsTransductive(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                                   max_hyperedge_size=max_groupsize,\n",
    "                                                   feature_dimension=feature_dimension,\n",
    "                                                   num_outputs=num_categories,\n",
    "                                                   dataset_name=dataset_name)\n",
    "plot_model(Deepsets_transductive_model, to_file='images/'+dataset_name+'_Deepsets_transductive_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/Deepsets_transductive_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/Deepsets_transductive_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = Deepsets_transductive_model.fit(X_deepset_transductive_train, Y_deepset_transductive_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.78122,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_deepsets = Deepsets_transductive_model.predict(X_deepset_transductive_test, batch_size=16, verbose=1)\n",
    "finals_pred_deepsets = []\n",
    "finals_test_deepsets = []\n",
    "for p in Y_pred_deepsets:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_deepsets.append(final)\n",
    "\n",
    "for i in Y_deepset_transductive_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_deepsets.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_deepsets, finals_pred_deepsets, target_names=target_names,digits=4))\n",
    "print(accuracy_score(finals_test_deepsets, finals_pred_deepsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from Models import DeepHyperedgesTransductive\n",
    "from Models import MLPTransductive\n",
    "from Models import DeepSetsTransductive\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"cora_transductive\"\n",
    "\n",
    "deephyperedges_transductive_model = DeepHyperedgesTransductive(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                                                hyperedge_embedding_dimension=hyperedge_embedding_dimension, \n",
    "                                                                feature_dimension=feature_dimension,\n",
    "                                                                max_hyperedge_size=max_groupsize,\n",
    "                                                                num_outputs=num_categories,\n",
    "                                                                dataset_name=dataset_name)\n",
    "deephyperedges_transductive_model.save_weights('models/'+dataset_name+'/deephyperedges_transductive_model.h5')\n",
    "\n",
    "MLP_transductive_model = MLPTransductive(hyperedge_embedding_dimension=hyperedge_embedding_dimension,\n",
    "                                            feature_dimension=feature_dimension,\n",
    "                                            num_outputs=num_categories,\n",
    "                                            dataset_name=dataset_name)\n",
    "MLP_transductive_model.save_weights('models/'+dataset_name+'/MLP_transductive_model.h5')\n",
    "\n",
    "deepsets_transductive_model = DeepSetsTransductive(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                                       max_hyperedge_size=max_groupsize,\n",
    "                                                       feature_dimension=feature_dimension,\n",
    "                                                       num_outputs=num_categories,\n",
    "                                                       dataset_name=dataset_name)\n",
    "deepsets_transductive_model.save_weights('models/'+dataset_name+'/deepsets_transductive_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "batch_size = 16\n",
    "\n",
    "def hyperedgesTrain(X_train,Y_train,num_epochs):\n",
    "    deephyperedges_transductive_model.load_weights('models/'+dataset_name+'/deephyperedges_transductive_model.h5')\n",
    "    history = deephyperedges_transductive_model.fit(X_train, Y_train, epochs=num_epochs, batch_size=batch_size,\n",
    "            shuffle=True, validation_split=0,\n",
    "            verbose=0)\n",
    "\n",
    "def MLPTrain(X_MLP_transductive_train, Y_MLP_transductive_train,num_epochs):\n",
    "    MLP_transductive_model.load_weights('models/'+dataset_name+'/MLP_transductive_model.h5')\n",
    "    history = MLP_transductive_model.fit(X_MLP_transductive_train, Y_MLP_transductive_train, epochs=num_epochs, batch_size=batch_size,\n",
    "            shuffle=True, validation_split=0,\n",
    "            verbose=0)\n",
    "    \n",
    "def DeepSetsTrain(X_deepset_transductive_train,Y_deepset_transductive_train,num_epochs):\n",
    "    deepsets_transductive_model.load_weights('models/'+dataset_name+'/deepsets_transductive_model.h5')\n",
    "    history = deepsets_transductive_model.fit(X_deepset_transductive_train, Y_deepset_transductive_train, epochs=num_epochs, batch_size=batch_size,\n",
    "            shuffle=True, validation_split=0,\n",
    "            verbose=0)\n",
    "    \n",
    "def testModel(model,X_tst,Y_tst):\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "    y_pred = model.predict(X_tst, batch_size=16, verbose=0)\n",
    "    finals_pred = []\n",
    "    finals_test = []\n",
    "    for p in y_pred:\n",
    "        m = 0\n",
    "        ind = 0\n",
    "        final = 0\n",
    "        for i in p:\n",
    "            if i>m:\n",
    "                m=i\n",
    "                final=ind\n",
    "            ind+=1\n",
    "        finals_pred.append(final)\n",
    "\n",
    "    for i in Y_tst:\n",
    "        ind=0\n",
    "        for j in i:\n",
    "            if j==1:\n",
    "                finals_test.append(ind)\n",
    "            ind+=1\n",
    "    c = classification_report(finals_test, finals_pred, target_names=target_names,digits=4)\n",
    "    reports.append(c)\n",
    "    print(c)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "def RunAllTests(percentTraining, num_times, num_epochs):\n",
    "    for i in range(num_times):\n",
    "        \n",
    "        print(\"percent: \",percentTraining,\", iteration: \",i+1,\", model: deep hyperedges\")\n",
    "        X, Y = getTrainingData()\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=percentTraining, test_size=1-percentTraining)\n",
    "        hyperedgesTrain(X_train,Y_train,num_epochs)\n",
    "        testModel(deephyperedges_transductive_model,X_test,Y_test)\n",
    "\n",
    "#         print(\"percent: \",percentTraining,\", iteration: \",i+1,\", model: MLP\")\n",
    "#         X_MLP, Y_MLP = getMLPTrainingData()\n",
    "#         X_MLP_transductive_train, X_MLP_transductive_test, Y_MLP_transductive_train, Y_MLP_transductive_test = train_test_split(X_MLP, Y_MLP, train_size=percentTraining, test_size=1-percentTraining)\n",
    "#         MLPTrain(X_MLP_transductive_train, Y_MLP_transductive_train,num_epochs)\n",
    "#         testModel(MLP_transductive_model,X_MLP_transductive_test,Y_MLP_transductive_test)\n",
    "\n",
    "#         print(\"percent: \",percentTraining,\", iteration: \",i+1,\", model: deep sets\")\n",
    "#         X_deepset, Y_deepset = getDSTrainingData()\n",
    "#         X_deepset_transductive_train, X_deepset_transductive_test, Y_deepset_transductive_train, Y_deepset_transductive_test = train_test_split(X_deepset, Y_deepset, train_size=percentTraining, test_size=1-percentTraining)\n",
    "#         DeepSetsTrain(X_deepset_transductive_train,Y_deepset_transductive_train,num_epochs)\n",
    "#         testModel(deepsets_transductive_model,X_deepset_transductive_test,Y_deepset_transductive_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.1, 5, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.3, 5, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.5, 5, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.1, 5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.3, 5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.5, 5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.1, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.3, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunAllTests(0.5, 5, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
