{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/cora\"\n",
    "max_groupsize = 169\n",
    "min_groupsize = 2\n",
    "num_categories = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(datadir+\"/cora.edges\", nodetype=str) # must be in edgelist (node1 node2\\n) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "with open(datadir+\"/cora.node_labels\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "for line in content:\n",
    "    keyval = line.split(\",\")\n",
    "    classes[str(keyval[0])] = str(keyval[1][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges = {}\n",
    "nodes = G.nodes\n",
    "max_size = 0\n",
    "min_size = 100000\n",
    "for node in nodes:\n",
    "    neighborhood = list(G.neighbors(node))\n",
    "    if len(neighborhood)+1 <= max_groupsize and len(neighborhood)+1 >= min_groupsize:\n",
    "        hyperedges[node] = {}\n",
    "        hyperedges[node][\"members\"] = []\n",
    "        hyperedges[node][\"members\"].append(node)\n",
    "        hyperedges[node][\"category\"] = classes[node]\n",
    "        for neighbor in neighborhood:\n",
    "            hyperedges[node][\"members\"].append(neighbor)\n",
    "    if len(neighborhood)+1 < min_size:\n",
    "        min_size = len(neighborhood)+1\n",
    "    if len(neighborhood) > max_size:\n",
    "        max_size = len(neighborhood)+1\n",
    "print(min_size, max_size)\n",
    "print(len(hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_amounts = {}\n",
    "cat_names = {\"1\":\"Neural Networks\",\"2\":\"Case Based\",\"3\":\"Reinforcement Learning\", \"4\":\"Probabilistic Methods\", \"5\":\"Genetic Algorithms\", \"6\":\"Rule Learning\", \"7\": \"Theory\"}\n",
    "\n",
    "for h in hyperedges:\n",
    "    if cat_names[str(hyperedges[h][\"category\"])] not in cat_amounts:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]=1\n",
    "    else:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]+=1\n",
    "\n",
    "\n",
    "pd_df = pd.DataFrame(list(cat_amounts.items()))\n",
    "pd_df.columns =[\"Dim\",\"Count\"]\n",
    "# sort df by Count column\n",
    "pd_df = pd_df.sort_values(['Count']).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# plot barh chart with index as x values\n",
    "ax = sns.barplot(pd_df.index, pd_df.Count)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "ax.set(xlabel=\"Dim\", ylabel='Count')\n",
    "# add proper Dim values as x labels\n",
    "ax.set_xticklabels(pd_df.Dim)\n",
    "for item in ax.get_xticklabels(): item.set_rotation(90)\n",
    "for i, v in enumerate(pd_df[\"Count\"].iteritems()):        \n",
    "    ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexMemberships = {}\n",
    "for h_index in hyperedges:\n",
    "    hyperedge = hyperedges[h_index]\n",
    "    authors = hyperedge[\"members\"]\n",
    "    for author in authors:\n",
    "        if author in vertexMemberships:\n",
    "            vertexMemberships[author].append(h_index)\n",
    "        else:\n",
    "            authorMembershipList = []\n",
    "            authorMembershipList.append(h_index)\n",
    "            vertexMemberships[author] = authorMembershipList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hyperedges))\n",
    "print(len(vertexMemberships))\n",
    "with open(datadir+'/hyperedges.p', 'wb') as fp:\n",
    "    pickle.dump(hyperedges, fp)\n",
    "with open(datadir+'/vertexMemberships.p', 'wb') as fp:\n",
    "    pickle.dump(vertexMemberships, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import SubsampleAndTraverse\n",
    "\n",
    "walksSAT = SubsampleAndTraverse(length=25, \n",
    "                                   num_walks=25, \n",
    "                                   hyperedges=hyperedges, \n",
    "                                   vertexMemberships=vertexMemberships,\n",
    "                                   alpha=1.,\n",
    "                                   beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import TraverseAndSelect\n",
    "\n",
    "walksTAS = TraverseAndSelect(length=25, \n",
    "                               num_walks=25, \n",
    "                               hyperedges=hyperedges, \n",
    "                               vertexMemberships=vertexMemberships,\n",
    "                               alpha=1.,\n",
    "                               beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir+'/walksSAT.p', 'wb') as fp:\n",
    "    pickle.dump(walksSAT, fp)\n",
    "\n",
    "with open(datadir+'/walksTAS.p', 'wb') as fp:\n",
    "    pickle.dump(walksTAS, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import EmbedWord2Vec\n",
    "\n",
    "vertex_embedding_dimension = 16\n",
    "hyperedge_embedding_dimension = 128\n",
    "\n",
    "vertex_ids, vertex_embeddings = EmbedWord2Vec(walks=walksSAT,dimension=vertex_embedding_dimension)\n",
    "print(\"Vertex embeddings finished.\")\n",
    "hyperedge_ids, hyperedge_embeddings = EmbedWord2Vec(walks=walksTAS,dimension=hyperedge_embedding_dimension)\n",
    "print(\"Hyperedge embeddings finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vertex_embeddings))\n",
    "with open(datadir+'/vertex_embeddings_'+str(max_groupsize)+'.p', 'wb') as fp:\n",
    "    pickle.dump(vertex_embeddings, fp)\n",
    "print(len(hyperedge_embeddings))\n",
    "with open(datadir+'/hyperedge_embeddings_'+str(max_groupsize)+'.p', 'wb') as fp:\n",
    "    pickle.dump(hyperedge_embeddings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "def visualizeTSNE(embeddings,obj,with_labels=None,ids=None):\n",
    "    targets = None\n",
    "    transform = TSNE\n",
    "    num_cats=7\n",
    "    trans = transform(n_components=2)\n",
    "    embeddings_2d = trans.fit_transform(embeddings)\n",
    "\n",
    "    alpha = 0.7\n",
    "    plt.figure(figsize=(8,7))\n",
    "    \n",
    "    if with_labels:\n",
    "        targets = [int(with_labels[identifier]['category'])-1 for identifier in ids]\n",
    "        col = [\"black\",\"blue\",\"turquoise\",\"gold\",\"red\",\"magenta\",\"darkgreen\"]\n",
    "#         col = [\"yellow\",\"yellowgreen\",\"midnightblue\",\"teal\",\"royalblue\",\"indigo\",\"mediumseagreen\"]\n",
    "        cmap1 = ListedColormap(col)\n",
    "        cmap = cmap1(np.linspace(0.0, 1.0, 7))\n",
    "        \n",
    "        print(type(cmap))\n",
    "        colors = cmap[targets]\n",
    "        \n",
    "        plt.scatter(embeddings_2d[:,0], \n",
    "                    embeddings_2d[:,1], \n",
    "                    cmap=\"jet\", c=colors, alpha=alpha, marker=\"+\")\n",
    "    else:\n",
    "        plt.scatter(embeddings_2d[:,0], \n",
    "                    embeddings_2d[:,1], \n",
    "                    cmap=\"jet\", alpha=alpha, marker=\"+\")\n",
    "    \n",
    "#     plt.title('TSNE visualization of '+obj+' embeddings in the hypergraph.'.format(transform.__name__))\n",
    "#     plt.savefig(\"tsne.png\",dpi=200)\n",
    "    plt.show()\n",
    "    return targets\n",
    "# targets = visualizeTSNE(vertex_embeddings,\"vertex\")\n",
    "targets = visualizeTSNE(hyperedge_embeddings,\"hyperedge\",hyperedges,hyperedge_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list,\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((hyperedge_embedding_dimension + vertex_embedding_dimension*max_groupsize,))\n",
    "        i = 0\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        \n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[hyperedge_embedding_dimension + i*embedding.shape[0]:hyperedge_embedding_dimension + (i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = getTrainingData()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# np.save(datadir+'/X_hyperedge_'+str(max_groupsize)+'.npy', X)\n",
    "# np.save(datadir+'/Y_hyperedge_'+str(max_groupsize)+'.npy', Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Models import DeepHyperedges\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"cora\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "deephyperedges_model = DeepHyperedges(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                     hyperedge_embedding_dimension=hyperedge_embedding_dimension,\n",
    "                                     max_hyperedge_size=max_groupsize,\n",
    "                                     num_outputs=num_categories,\n",
    "                                     dataset_name=dataset_name)\n",
    "plot_model(deephyperedges_model, to_file='images/'+dataset_name+'_deephyperedges_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/deephyperedges_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/deephyperedges_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = deephyperedges_model.fit(X_train, Y_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "\n",
    "y_pred = deephyperedges_model.predict(X_test, batch_size=16, verbose=1)\n",
    "finals_pred = []\n",
    "finals_test = []\n",
    "for p in y_pred:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred.append(final)\n",
    "\n",
    "for i in Y_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test, finals_pred, target_names=target_names))\n",
    "print(accuracy_score(finals_test, finals_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLPTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        lists.append({\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    for hlist in lists:\n",
    "        x = np.zeros((hyperedge_embedding_dimension,))\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X_MLP = np.asarray(X_arr)\n",
    "    Y_MLP = np.asarray(Y_arr)\n",
    "    return X_MLP, Y_MLP\n",
    "\n",
    "X_MLP, Y_MLP = getMLPTrainingData()\n",
    "# np.save(datadir+'/X_MLP_'+str(max_groupsize)+'.npy', X_MLP)\n",
    "# np.save(datadir+'/Y_MLP_'+str(max_groupsize)+'.npy', Y_MLP)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_MLP_train, X_MLP_test, Y_MLP_train, Y_MLP_test = train_test_split(X_MLP, Y_MLP, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from Models import MLP\n",
    "dataset_name = \"cora\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "MLP_model = MLP(input_dimension=hyperedge_embedding_dimension,\n",
    "                 num_outputs=num_categories,\n",
    "                 dataset_name=dataset_name)\n",
    "plot_model(MLP_model, to_file='images/'+dataset_name+'_MLP_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/MLP_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/MLP_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = MLP_model.fit(X_MLP_train, Y_MLP_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.1,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Neural Networks\",\"Case Based\",\"Reinforcement Learning\",\"Probabilistic Methods\",\"Genetic Algorithms\",\"Rule Learning\",\"Theory\"]\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "Y_pred_MLP = MLP_model.predict(X_MLP_test, batch_size=16, verbose=1)\n",
    "finals_pred_MLP = []\n",
    "finals_test_MLP = []\n",
    "for p in Y_pred_MLP:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_MLP.append(final)\n",
    "\n",
    "for i in Y_MLP_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_MLP.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_MLP, finals_pred_MLP, target_names=target_names))\n",
    "print(accuracy_score(finals_test_MLP, finals_pred_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((vertex_embedding_dimension*max_groupsize,))\n",
    "        i = 0        \n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[i*embedding.shape[0]:(i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X_deepset, Y_deepset = getTrainingData()\n",
    "\n",
    "print(X_deepset.shape)\n",
    "print(Y_deepset.shape)\n",
    "\n",
    "# np.save(datadir+'/X_deepset_'+str(max_groupsize)+'.npy', X_deepset)\n",
    "# np.save(datadir+'/Y_deepset_'+str(max_groupsize)+'.npy', Y_deepset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_deepset_train, X_deepset_test, Y_deepset_train, Y_deepset_test = train_test_split(X_deepset, Y_deepset, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from Models import DeepSets\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"cora\"\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "deepsets_model = DeepSets(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                             max_hyperedge_size=max_groupsize,\n",
    "                             num_outputs=num_categories,\n",
    "                             dataset_name=dataset_name)\n",
    "plot_model(deepsets_model, to_file='images/'+dataset_name+'_deepsets_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/deepsets_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/deepsets_logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = deepsets_model.fit(X_deepset_train, Y_deepset_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.1,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_deepsets = deepsets_model.predict(X_deepset_test, batch_size=16, verbose=1)\n",
    "finals_pred_deepsets = []\n",
    "finals_test_deepsets = []\n",
    "for p in Y_pred_deepsets:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_deepsets.append(final)\n",
    "\n",
    "for i in Y_deepset_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_deepsets.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_deepsets, finals_pred_deepsets, target_names=target_names))\n",
    "print(accuracy_score(finals_test_deepsets, finals_pred_deepsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
