{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/corum\"\n",
    "max_groupsize = 143\n",
    "min_groupsize = 1\n",
    "num_negatives = 4000\n",
    "num_categories = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexes = pd.read_csv(datadir+'/allComplexes.txt',delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges = {}\n",
    "cardinalities = {\"2\":0,\"3\":0,\"4\":0,\"5-6\":0,\"7-9\":0,\"10-14\":0,\"15-19\":0,\"20-29\":0,\"30-49\":0,\"50-79\":0,\"80-150\":0}\n",
    "allProteins = []\n",
    "max_size = 0\n",
    "min_size = 100000\n",
    "for index, row in complexes.iterrows():\n",
    "    group_id = str(row[\"ComplexID\"])\n",
    "    proteins = row[\"subunits(UniProt IDs)\"].split(\";\")\n",
    "    for protein in proteins:\n",
    "        if protein not in allProteins: # terribly inefficient, I know\n",
    "            allProteins.append(protein)\n",
    "    if len(proteins) <= max_groupsize and len(proteins) >= min_groupsize:\n",
    "        if len(proteins)==2:\n",
    "            cardinalities[\"2\"]+=1\n",
    "        if len(proteins)==3:\n",
    "            cardinalities[\"3\"]+=1\n",
    "        if len(proteins)==4:\n",
    "            cardinalities[\"4\"]+=1\n",
    "        if len(proteins)>=5 and len(proteins)<=6:\n",
    "            cardinalities[\"5-6\"]+=1\n",
    "        if len(proteins)>=7 and len(proteins)<=9:\n",
    "            cardinalities[\"7-9\"]+=1\n",
    "        if len(proteins)>=10 and len(proteins)<=14:\n",
    "            cardinalities[\"10-14\"]+=1\n",
    "        if len(proteins)>=15 and len(proteins)<=19:\n",
    "            cardinalities[\"15-19\"]+=1\n",
    "        if len(proteins)>=20 and len(proteins)<=29:\n",
    "            cardinalities[\"20-29\"]+=1\n",
    "        if len(proteins)>=30 and len(proteins)<=49:\n",
    "            cardinalities[\"30-49\"]+=1\n",
    "        if len(proteins)>=50 and len(proteins)<=79:\n",
    "            cardinalities[\"50-79\"]+=1\n",
    "        if len(proteins)>=80 and len(proteins)<=150:\n",
    "            cardinalities[\"80-150\"]+=1\n",
    "        \n",
    "        hyperedges[group_id] = {}\n",
    "        hyperedges[group_id][\"members\"] = proteins\n",
    "        hyperedges[group_id][\"category\"] = \"1\"\n",
    "    if len(proteins) < min_size:\n",
    "        min_size = len(proteins)\n",
    "    if len(proteins) > max_size:\n",
    "        max_size = len(proteins)    \n",
    "print(min_size, max_size)\n",
    "print(len(allProteins))\n",
    "print(cardinalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Generate inside distribution of cardinalities\n",
    "\n",
    "# i = 0\n",
    "# for _ in range(cardinalities[\"2\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, 2)\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"3\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, 3)\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"4\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, 4)\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"5-6\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(5,6))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"7-9\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(7,9))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"10-14\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(10,14))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"15-19\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(15,19))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"20-29\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(20,29))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"30-49\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(30,49))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"50-79\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(50,79))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1\n",
    "# for _ in range(cardinalities[\"80-150\"]):\n",
    "#     hyperedges[\"neg_\"+str(i)] = {}\n",
    "#     hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(80,150))\n",
    "#     hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\"\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate uniform distribution of cardinalities\n",
    "for i in range(num_negatives):\n",
    "    hyperedges[\"neg_\"+str(i)] = {}\n",
    "    hyperedges[\"neg_\"+str(i)][\"members\"] = np.random.choice(allProteins, random.randint(min_groupsize,max_groupsize))\n",
    "    hyperedges[\"neg_\"+str(i)][\"category\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_amounts = {}\n",
    "cat_names = {\"0\":\"No complex formed\",\"1\":\"Forms complex\"}\n",
    "\n",
    "for h in hyperedges:\n",
    "    if cat_names[str(hyperedges[h][\"category\"])] not in cat_amounts:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]=1\n",
    "    else:\n",
    "        cat_amounts[cat_names[str(hyperedges[h][\"category\"])]]+=1\n",
    "\n",
    "\n",
    "pd_df = pd.DataFrame(list(cat_amounts.items()))\n",
    "pd_df.columns =[\"Dim\",\"Count\"]\n",
    "# sort df by Count column\n",
    "pd_df = pd_df.sort_values(['Count']).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# plot barh chart with index as x values\n",
    "ax = sns.barplot(pd_df.index, pd_df.Count)\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "ax.set(xlabel=\"Dim\", ylabel='Count')\n",
    "# add proper Dim values as x labels\n",
    "ax.set_xticklabels(pd_df.Dim)\n",
    "for item in ax.get_xticklabels(): item.set_rotation(90)\n",
    "for i, v in enumerate(pd_df[\"Count\"].iteritems()):        \n",
    "    ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexMemberships = {}\n",
    "for h_index in hyperedges:\n",
    "    hyperedge = hyperedges[h_index]\n",
    "    proteins = hyperedge[\"members\"]\n",
    "    for protein in proteins:\n",
    "        if protein in vertexMemberships:\n",
    "            vertexMemberships[protein].append(h_index)\n",
    "        else:\n",
    "            proteinMembershipList = []\n",
    "            proteinMembershipList.append(h_index)\n",
    "            vertexMemberships[protein] = proteinMembershipList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hyperedges))\n",
    "print(len(vertexMemberships))\n",
    "with open(datadir+'/hyperedges.p', 'wb') as fp:\n",
    "    pickle.dump(hyperedges, fp)\n",
    "with open(datadir+'/vertexMemberships.p', 'wb') as fp:\n",
    "    pickle.dump(vertexMemberships, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import SubsampleAndTraverse\n",
    "\n",
    "walksSAT = SubsampleAndTraverse(length=50, \n",
    "                                   num_walks=50, \n",
    "                                   hyperedges=hyperedges, \n",
    "                                   vertexMemberships=vertexMemberships,\n",
    "                                   alpha=1.,\n",
    "                                   beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HypergraphRandomWalks import TraverseAndSelect\n",
    "\n",
    "walksTAS = TraverseAndSelect(length=50, \n",
    "                               num_walks=50, \n",
    "                               hyperedges=hyperedges, \n",
    "                               vertexMemberships=vertexMemberships,\n",
    "                               alpha=1.,\n",
    "                               beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir+'/walksSAT.p', 'wb') as fp:\n",
    "    pickle.dump(walksSAT, fp)\n",
    "\n",
    "with open(datadir+'/walksTAS.p', 'wb') as fp:\n",
    "    pickle.dump(walksTAS, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import EmbedWord2Vec\n",
    "\n",
    "vertex_embedding_dimension = 16\n",
    "hyperedge_embedding_dimension = 128\n",
    "\n",
    "vertex_ids, vertex_embeddings = EmbedWord2Vec(walks=walksSAT,dimension=vertex_embedding_dimension)\n",
    "print(\"Vertex embeddings finished.\")\n",
    "hyperedge_ids, hyperedge_embeddings = EmbedWord2Vec(walks=walksTAS,dimension=hyperedge_embedding_dimension)\n",
    "print(\"Hyperedge embeddings finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vertex_embeddings))\n",
    "with open(datadir+'/vertex_embeddings_'+str(max_groupsize)+'.p', 'wb') as fp:\n",
    "    pickle.dump(vertex_embeddings, fp)\n",
    "print(len(hyperedge_embeddings))\n",
    "with open(datadir+'/hyperedge_embeddings_'+str(max_groupsize)+'.p', 'wb') as fp:\n",
    "    pickle.dump(hyperedge_embeddings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualizeTSNE(embeddings,obj,with_labels=None,ids=None):\n",
    "    transform = TSNE\n",
    "\n",
    "    trans = transform(n_components=2)\n",
    "    embeddings_2d = trans.fit_transform(embeddings)\n",
    "\n",
    "    alpha = 0.7\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.axes().set(aspect=\"equal\")\n",
    "    \n",
    "    if with_labels:\n",
    "        targets = [int(with_labels[identifier]['category']) for identifier in ids]\n",
    "        cmap = cm.rainbow(np.linspace(0.0, 1.0, 2))\n",
    "        colors = cmap[targets]\n",
    "        \n",
    "        plt.scatter(embeddings_2d[:,0], \n",
    "                    embeddings_2d[:,1], \n",
    "                    cmap=\"jet\", c=colors, alpha=alpha)\n",
    "    else:\n",
    "        plt.scatter(embeddings_2d[:,0], \n",
    "                    embeddings_2d[:,1], \n",
    "                    cmap=\"jet\", alpha=alpha)\n",
    "    \n",
    "    plt.title('TSNE visualization of '+obj+' embeddings in the hypergraph.'.format(transform.__name__))\n",
    "    plt.show()\n",
    "    \n",
    "visualizeTSNE(vertex_embeddings,\"vertex\")\n",
    "visualizeTSNE(hyperedge_embeddings,\"hyperedge\",hyperedges,hyperedge_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        \n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list,\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((hyperedge_embedding_dimension + vertex_embedding_dimension*max_groupsize,))\n",
    "        i = 0\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        \n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[hyperedge_embedding_dimension + i*embedding.shape[0]:hyperedge_embedding_dimension + (i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = getTrainingData()\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# np.save(datadir+'/X_hyperedge_'+str(max_groupsize)+'.npy', X)\n",
    "# np.save(datadir+'/Y_hyperedge_'+str(max_groupsize)+'.npy', Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Models import DeepHyperedges\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"corum\"\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "deephyperedges_model = DeepHyperedges(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                                     hyperedge_embedding_dimension=hyperedge_embedding_dimension,\n",
    "                                     max_hyperedge_size=max_groupsize,\n",
    "                                     num_outputs=num_categories,\n",
    "                                     dataset_name=dataset_name)\n",
    "plot_model(deephyperedges_model, to_file='images/'+dataset_name+'_deephyperedges_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/deephyperedges_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/deephyperedges_logs_diff', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = deephyperedges_model.fit(X_train, Y_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.1,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "target_names = [\"No protein complex formed\",\"Protein complex formed\"]\n",
    "\n",
    "y_pred = deephyperedges_model.predict(X_test, batch_size=16, verbose=1)\n",
    "finals_pred = []\n",
    "finals_test = []\n",
    "for p in y_pred:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred.append(final)\n",
    "\n",
    "for i in Y_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test, finals_pred, target_names=target_names))\n",
    "print(accuracy_score(finals_test, finals_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMLPTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        lists.append({\"h\":hyperedge_embeddings[hyperedge_ids.index(h)].tolist()})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    for hlist in lists:\n",
    "        x = np.zeros((hyperedge_embedding_dimension,))\n",
    "        x[:hyperedge_embedding_dimension] = hlist[\"h\"]\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X_MLP = np.asarray(X_arr)\n",
    "    Y_MLP = np.asarray(Y_arr)\n",
    "    return X_MLP, Y_MLP\n",
    "\n",
    "X_MLP, Y_MLP = getMLPTrainingData()\n",
    "# np.save(datadir+'/X_MLP_'+str(max_groupsize)+'.npy', X_MLP)\n",
    "# np.save(datadir+'/Y_MLP_'+str(max_groupsize)+'.npy', Y_MLP)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_MLP_train, X_MLP_test, Y_MLP_train, Y_MLP_test = train_test_split(X_MLP, Y_MLP, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from Models import MLP\n",
    "dataset_name = \"corum\"\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "MLP_model = MLP(input_dimension=hyperedge_embedding_dimension,\n",
    "                 num_outputs=num_categories,\n",
    "                 dataset_name=dataset_name)\n",
    "plot_model(MLP_model, to_file='images/'+dataset_name+'_MLP_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/MLP_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/MLP_logs_diff', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = MLP_model.fit(X_MLP_train, Y_MLP_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.1,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_MLP = MLP_model.predict(X_MLP_test, batch_size=16, verbose=1)\n",
    "finals_pred_MLP = []\n",
    "finals_test_MLP = []\n",
    "for p in Y_pred_MLP:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_MLP.append(final)\n",
    "\n",
    "for i in Y_MLP_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_MLP.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_MLP, finals_pred_MLP, target_names=target_names))\n",
    "print(accuracy_score(finals_test_MLP, finals_pred_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    i = 0\n",
    "    lists = []\n",
    "    labels = []\n",
    "    maxi = 0\n",
    "    for h in hyperedges:\n",
    "        vertex_embedding_list = []\n",
    "        hyperedge = hyperedges[h]\n",
    "        for vertex in hyperedge[\"members\"]:\n",
    "            i+=1\n",
    "            if i%100000==0:\n",
    "                print(i)\n",
    "            try: # Good ol' nondeterminism\n",
    "                vertex_embedding_list.append(vertex_embeddings[vertex_ids.index(vertex)].tolist())\n",
    "            except:\n",
    "                print(\"Missed one: \",vertex)\n",
    "        lists.append({\"v\":vertex_embedding_list})\n",
    "        label = np.zeros((num_categories,))\n",
    "        label[int(hyperedge[\"category\"])-1] = 1\n",
    "        labels.append(label)\n",
    "    X_unshuffled = []\n",
    "    \n",
    "    for hlist in lists:\n",
    "        np_vertex_embeddings = np.asarray(hlist[\"v\"])\n",
    "        x = np.zeros((vertex_embedding_dimension*max_groupsize,))\n",
    "        i = 0        \n",
    "        for embedding in np_vertex_embeddings:\n",
    "            x[i*embedding.shape[0]:(i+1)*embedding.shape[0]] = embedding\n",
    "            i+=1\n",
    "        X_unshuffled.append(x)\n",
    "    labels = np.asarray(labels)\n",
    "    X_arr, Y_arr = shuffle(X_unshuffled, labels)\n",
    "    X = np.asarray(X_arr)\n",
    "    Y = np.asarray(Y_arr)\n",
    "    return X, Y\n",
    "\n",
    "X_deepset, Y_deepset = getTrainingData()\n",
    "\n",
    "print(X_deepset.shape)\n",
    "print(Y_deepset.shape)\n",
    "\n",
    "# np.save(datadir+'/X_deepset_'+str(max_groupsize)+'.npy', X_deepset)\n",
    "# np.save(datadir+'/Y_deepset_'+str(max_groupsize)+'.npy', Y_deepset)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_deepset_train, X_deepset_test, Y_deepset_train, Y_deepset_test = train_test_split(X_deepset, Y_deepset, train_size=0.9, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from Models import DeepSets\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "dataset_name = \"corum\"\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "deepsets_model = DeepSets(vertex_embedding_dimension=vertex_embedding_dimension,\n",
    "                             max_hyperedge_size=max_groupsize,\n",
    "                             num_outputs=num_categories,\n",
    "                             dataset_name=dataset_name)\n",
    "plot_model(deepsets_model, to_file='images/'+dataset_name+'_deepsets_model.png')\n",
    "checkpointer = ModelCheckpoint(filepath='weights/'+dataset_name+'/deepsets_weights.hdf5', verbose=0, save_best_only=True)\n",
    "tbCallBack = TensorBoard(log_dir='logs/'+dataset_name+'/deepsets_logs_diff', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, update_freq='batch')\n",
    "\n",
    "history = deepsets_model.fit(X_deepset_train, Y_deepset_train, epochs=num_epochs, batch_size=batch_size,\n",
    "        shuffle=True, validation_split=0.1,\n",
    "        callbacks=[checkpointer,tbCallBack], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_deepsets = deepsets_model.predict(X_deepset_test, batch_size=16, verbose=1)\n",
    "finals_pred_deepsets = []\n",
    "finals_test_deepsets = []\n",
    "for p in Y_pred_deepsets:\n",
    "    m = 0\n",
    "    ind = 0\n",
    "    final = 0\n",
    "    for i in p:\n",
    "        if i>m:\n",
    "            m=i\n",
    "            final=ind\n",
    "        ind+=1\n",
    "    finals_pred_deepsets.append(final)\n",
    "\n",
    "for i in Y_deepset_test:\n",
    "    ind=0\n",
    "    for j in i:\n",
    "        if j==1:\n",
    "            finals_test_deepsets.append(ind)\n",
    "        ind+=1\n",
    "            \n",
    "print(classification_report(finals_test_deepsets, finals_pred_deepsets, target_names=target_names))\n",
    "print(accuracy_score(finals_test_deepsets, finals_pred_deepsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
